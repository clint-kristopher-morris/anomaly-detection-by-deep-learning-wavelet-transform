{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to Parquet\n",
    "Instead, we’ll store our data in Parquet, a format that is more efficient for computers to read and write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:38229</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>16.69 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:38229' processes=4 threads=8, memory=16.69 GB>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "import fastparquet \n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time2Mins(time_):\n",
    "    hr2min, mins = time_.split(':')\n",
    "    return ((int(hr2min)*60)+int(mins))/5\n",
    "\n",
    "def date2Mins(date):\n",
    "    date_format = \"%m/%d/%Y\"\n",
    "    d0 = datetime.strptime(str(date), date_format)\n",
    "    d1 = datetime.strptime('07/31/2019', date_format)\n",
    "    delta = d0 - d1\n",
    "    return (int((delta.days)*24*60))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Partitions should fit comfortably in memory (smaller than a gigabyte) \n",
    "but also not be too many. Every operation on every partition takes the \n",
    "central scheduler a few hundred microseconds to process. If you have a \n",
    "few thousand tasks this is barely noticeable, but it is nice to reduce \n",
    "the number if possible.\n",
    "\n",
    "A common situation is that you load lots of data into reasonably sized \n",
    "partitions (Dask’s defaults make decent choices), but then you filter\n",
    "down your dataset to only a small fraction of the original. At this point,\n",
    "it is wise to regroup your many small partitions into a few larger ones. \n",
    "You can do this by using the repartition method:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# join 37 files into a single df\n",
    "station = '15799'\n",
    "ddf = dd.read_csv(f'./data/Targetted-GDOT/{station}/{station}_*.csv',assume_missing=True)\n",
    "ddf = ddf.repartition(npartitions=4)\n",
    "\n",
    "# selecting all numeric cols\n",
    "lane_type_idx = list(ddf.columns).index(\"lane_type\")\n",
    "data_names = list(ddf.columns[(lane_type_idx+1):])\n",
    "ddf=ddf.dropna()\n",
    "# for col in data_names:\n",
    "#     ddf[col] = ddf[col].fillna(100000)\n",
    "#     ddf[col] = ddf[col].replace('NA',10000)\n",
    "#     ddf[col] = ddf[col].replace('na',10000)\n",
    "#     ddf = ddf.replace('N/A',10000)\n",
    "\n",
    "# selecting all numeric cols\n",
    "# num_lanes = len(ddf[\"lane_type\"].unique())\n",
    "# print(num_lanes)\n",
    "# col2 = ddf.columns\n",
    "# lane_type_idx = list(ddf.columns).index(\"lane_type\")\n",
    "# data_names = list(ddf.columns[(lane_type_idx+1):])\n",
    "\n",
    "# # map columns and create new column\n",
    "# ddf.datestamp = ddf.datestamp.map(lambda x: date2Mins(x), meta=pd.Series([], dtype=str, name='x'))\n",
    "# ddf.timestamp = ddf.timestamp.map(lambda x: time2Mins(x), meta=pd.Series([], dtype=str, name='x'))\n",
    "# ddf = ddf.map_partitions(lambda df: df.assign(time_anchor=(df.datestamp + df.timestamp)))\n",
    "\n",
    "\n",
    "\n",
    "# # df can fit in mem so use persist\n",
    "# ddf = ddf.repartition(npartitions=1)\n",
    "# ddf = ddf.persist()\n",
    "# end = time.time()\n",
    "# print(f'1st process took: {end-start}')\n",
    "\n",
    "# start = time.time()\n",
    "# items = list(ddf['lane_type'].unique())\n",
    "# for lane in items:\n",
    "#     ddf[ddf['lane_type'] == lane].to_csv(f'./data/{lane}_*.csv')\n",
    "# end = time.time()\n",
    "# print(f'2nd process took: {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from termcolor import colored\n",
    "\n",
    "def skipfiles(start_at):\n",
    "    if skip == True:\n",
    "        if folder == start_at:\n",
    "            skip = False\n",
    "        else:\n",
    "            skip = True\n",
    "    return     \n",
    "\n",
    "def sort_process_ave(folders_dir, start_at=None, skip=False):\n",
    "    \n",
    "    folders = os.listdir(folders_dir)\n",
    "    folders.sort()\n",
    "    errors = []\n",
    "    for folder in folders:\n",
    "        \n",
    "        # if you need to process a segment at a time\n",
    "        if skip:\n",
    "            skip = skipfiles(start_at)\n",
    "            \n",
    "        print(folder)\n",
    "        if folder in ['2941']:\n",
    "            continue\n",
    "        # current version for single file\n",
    "        start = time.time()\n",
    "        \n",
    "        \n",
    "        # join 37 files into a single df\n",
    "#         print(f'{folders_dir}/{folder}/{folder}_*.csv')\n",
    "        ddf = dd.read_csv(f'{folders_dir}/{folder}/{folder}_*.csv', assume_missing=True)\n",
    "        ddf = ddf.repartition(npartitions=8)\n",
    "        \n",
    "        # selecting all numeric cols\n",
    "        lane_type_idx = list(ddf.columns).index(\"lane_type\")\n",
    "        data_names = list(ddf.columns[(lane_type_idx+1):])\n",
    "        for col in data_names:\n",
    "            c_name = col.split('_')\n",
    "            if 'spd' in c_name:\n",
    "                ddf[col] = ddf[col].fillna(-10000)\n",
    "        ddf=ddf.dropna()\n",
    "        \n",
    "#         ddf=ddf.fillna(-10000)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # get vName for csv label\n",
    "#         try:\n",
    "\n",
    "        try:\n",
    "            name = ddf['vName'].head(1).tolist()[0].split()[1]\n",
    "        except IndexError:\n",
    "            errors.append(folder)\n",
    "            print(colored('missing data at station file: ', None),colored(f'{folder}', 'red'))\n",
    "            continue    \n",
    "            \n",
    "#         name = (ddf['vName'].head(1)[0].split()[1])\n",
    "        num_lanes = len(ddf[\"lane_type\"].unique()) # get the number of lanes\n",
    "#         except:\n",
    "#             errors.append(folder)\n",
    "#             print(colored('Error at station file: ', None),colored(f'{folder}', 'red'))\n",
    "#             continue\n",
    "\n",
    "        \n",
    "        if folder != name:\n",
    "            print(f'non-match! folder: {folder}')\n",
    "            errors.append(folder)\n",
    "            print(colored('Error at station file: ', None),colored(f'{folder}', 'red'))\n",
    "            continue\n",
    "\n",
    "        # map columns and create new column\n",
    "        ddf.datestamp = ddf.datestamp.map(lambda x: date2Mins(x), meta=pd.Series([], dtype=str, name='x'))\n",
    "        ddf.timestamp = ddf.timestamp.map(lambda x: time2Mins(x), meta=pd.Series([], dtype=str, name='x'))\n",
    "        ddf = ddf.map_partitions(lambda df: df.assign(time_anchor=(df.datestamp + df.timestamp)))\n",
    "        ddf = ddf.groupby('time_anchor')[data_names].mean()\n",
    "        \n",
    "        for col in data_names:\n",
    "            c_name = col.split('_')\n",
    "            if any(x in c_name for x in ['spd','total','avg']):\n",
    "                pass\n",
    "            else:\n",
    "                column = '_'.join(col.split('_'))\n",
    "                ddf[column] = ddf[column].map(lambda x: x*num_lanes, meta=pd.Series([], dtype=str, name='x'))\n",
    "        ddf['det_avg_occ'] = ddf['det_avg_occ'].map(lambda x: x*num_lanes, meta=pd.Series([], dtype=str, name='x'))\n",
    "\n",
    "        # df can fit in mem so use persist\n",
    "        ddf = ddf.persist()\n",
    "\n",
    "        start = time.time()\n",
    "        ddf.to_csv(f'./data/{name}_*.csv')\n",
    "        end = time.time()\n",
    "        print(f'{folder} process took: {end-start}')\n",
    "        \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13414\n",
      "13414 process took: 1.9792237281799316\n",
      "13415\n",
      "13415 process took: 3.982919931411743\n",
      "13416\n",
      "13416 process took: 3.381715774536133\n",
      "13417\n",
      "13417 process took: 2.2020857334136963\n",
      "13419\n",
      "13419 process took: 2.8936495780944824\n",
      "13482\n",
      "13482 process took: 2.7302143573760986\n",
      "13508\n",
      "13508 process took: 3.1634011268615723\n",
      "13516\n",
      "13516 process took: 1.8067774772644043\n",
      "15799\n",
      "15799 process took: 0.893892765045166\n",
      "15801\n",
      "15801 process took: 1.795670986175537\n",
      "15802\n",
      "15802 process took: 1.776519775390625\n",
      "15805\n",
      "15805 process took: 0.5167829990386963\n",
      "15886\n",
      "missing data at station file: \u001b[0m \u001b[31m15886\u001b[0m\n",
      "15891\n",
      "15891 process took: 1.737846851348877\n",
      "15892\n",
      "15892 process took: 1.7567920684814453\n",
      "16019\n",
      "16019 process took: 3.61160945892334\n",
      "16020\n",
      "16020 process took: 5.193772315979004\n",
      "16043\n",
      "16043 process took: 3.3169870376586914\n",
      "202\n",
      "missing data at station file: \u001b[0m \u001b[31m202\u001b[0m\n",
      "2788\n",
      "2788 process took: 2.202224016189575\n",
      "2793\n",
      "2793 process took: 3.0816895961761475\n",
      "2797\n",
      "2797 process took: 4.405994415283203\n",
      "2798\n",
      "2798 process took: 6.55947208404541\n",
      "2809\n",
      "2809 process took: 5.472933530807495\n",
      "2810\n",
      "2810 process took: 3.165778160095215\n",
      "2821\n",
      "2821 process took: 3.7917354106903076\n",
      "2822\n",
      "2822 process took: 3.9903736114501953\n",
      "2826\n",
      "2826 process took: 4.142594337463379\n",
      "2827\n",
      "2827 process took: 3.8521485328674316\n",
      "2834\n",
      "2834 process took: 3.825925588607788\n",
      "2835\n",
      "2835 process took: 3.1608595848083496\n",
      "2836\n",
      "2836 process took: 3.1929194927215576\n",
      "2837\n",
      "2837 process took: 3.9061641693115234\n",
      "2852\n",
      "2852 process took: 3.790632724761963\n",
      "2855\n",
      "2855 process took: 3.39512038230896\n",
      "2856\n",
      "2856 process took: 3.6311371326446533\n",
      "2893\n",
      "2893 process took: 4.556631565093994\n",
      "2894\n",
      "2894 process took: 2.923428773880005\n",
      "2895\n",
      "2895 process took: 3.53749418258667\n",
      "2896\n",
      "2896 process took: 3.322702407836914\n",
      "2900\n",
      "2900 process took: 3.40079402923584\n",
      "2901\n",
      "2901 process took: 3.50610613822937\n",
      "2906\n",
      "2906 process took: 2.940775156021118\n",
      "2907\n",
      "2907 process took: 3.4154272079467773\n",
      "2914\n",
      "2914 process took: 6.216797590255737\n",
      "2915\n",
      "2915 process took: 6.8211236000061035\n",
      "2941\n",
      "2942\n",
      "2942 process took: 4.052067279815674\n",
      "2948\n",
      "2948 process took: 2.0443131923675537\n",
      "2979\n",
      "2979 process took: 3.3060810565948486\n",
      "2980\n",
      "2980 process took: 3.2658417224884033\n",
      "2998\n",
      "2998 process took: 3.285081148147583\n",
      "2999\n",
      "2999 process took: 3.990750312805176\n",
      "3007\n",
      "3007 process took: 3.545923948287964\n",
      "3157\n",
      "missing data at station file: \u001b[0m \u001b[31m3157\u001b[0m\n",
      "3173\n",
      "3173 process took: 4.590237379074097\n",
      "3174\n",
      "3174 process took: 4.634334325790405\n",
      "3208\n",
      "3208 process took: 4.03246545791626\n",
      "3209\n",
      "3209 process took: 4.163211345672607\n",
      "3226\n",
      "3226 process took: 4.447907209396362\n",
      "3227\n",
      "3227 process took: 4.579490423202515\n",
      "3232\n",
      "missing data at station file: \u001b[0m \u001b[31m3232\u001b[0m\n",
      "3233\n",
      "3233 process took: 3.836933135986328\n",
      "3234\n",
      "3234 process took: 5.1922266483306885\n",
      "3263\n",
      "3263 process took: 3.1790339946746826\n",
      "3266\n",
      "3266 process took: 4.138187885284424\n",
      "3271\n",
      "3271 process took: 5.11779522895813\n",
      "3280\n",
      "3280 process took: 3.794217348098755\n",
      "3285\n",
      "3285 process took: 4.82027792930603\n",
      "3309\n",
      "3309 process took: 6.3599560260772705\n",
      "3340\n",
      "3340 process took: 3.882869005203247\n",
      "3386\n",
      "3386 process took: 3.061347007751465\n",
      "3398\n",
      "3398 process took: 2.8192503452301025\n",
      "3417\n",
      "3417 process took: 1.7977476119995117\n",
      "3419\n",
      "3419 process took: 4.3494062423706055\n",
      "3420\n",
      "3420 process took: 3.8050124645233154\n",
      "3421\n",
      "3421 process took: 4.4343719482421875\n",
      "3441\n",
      "3441 process took: 2.266563653945923\n",
      "3444\n",
      "3444 process took: 3.2660794258117676\n",
      "3461\n",
      "3461 process took: 4.454363584518433\n",
      "3462\n",
      "3462 process took: 4.9838762283325195\n",
      "3477\n",
      "3477 process took: 2.306046485900879\n",
      "3478\n",
      "3478 process took: 2.428938627243042\n",
      "3490\n",
      "3490 process took: 3.562631607055664\n",
      "3491\n",
      "3491 process took: 3.2254135608673096\n",
      "3498\n",
      "3498 process took: 4.037152051925659\n",
      "3503\n",
      "3503 process took: 4.568291902542114\n",
      "3504\n",
      "3504 process took: 4.546164035797119\n",
      "3513\n",
      "3513 process took: 3.6389904022216797\n",
      "3522\n",
      "3522 process took: 2.9457385540008545\n",
      "3542\n",
      "3542 process took: 3.5595078468322754\n",
      "3544\n",
      "3544 process took: 3.852996826171875\n",
      "3545\n",
      "3545 process took: 3.708538293838501\n",
      "3550\n",
      "3550 process took: 3.6618294715881348\n",
      "3551\n",
      "3551 process took: 3.7730352878570557\n",
      "3553\n",
      "3553 process took: 3.6543726921081543\n",
      "3555\n",
      "3555 process took: 5.060065031051636\n",
      "3556\n",
      "3556 process took: 5.481770992279053\n",
      "3576\n",
      "3576 process took: 3.786402463912964\n",
      "3577\n",
      "3577 process took: 4.4450037479400635\n",
      "3609\n",
      "3609 process took: 3.053086519241333\n",
      "3621\n",
      "3621 process took: 6.144097805023193\n",
      "3622\n",
      "3622 process took: 3.757296085357666\n",
      "3633\n",
      "3633 process took: 3.9533748626708984\n",
      "3634\n",
      "3634 process took: 3.5902345180511475\n",
      "3651\n",
      "3651 process took: 3.2414495944976807\n",
      "3652\n",
      "3652 process took: 3.3492109775543213\n",
      "3653\n",
      "3653 process took: 3.3359408378601074\n",
      "3654\n",
      "3654 process took: 3.082601547241211\n",
      "3681\n",
      "3681 process took: 3.8640520572662354\n",
      "3682\n",
      "3682 process took: 3.5362942218780518\n",
      "3776\n",
      "3776 process took: 3.240969657897949\n",
      "3778\n",
      "3778 process took: 3.8082115650177\n",
      "3782\n",
      "3782 process took: 3.5833098888397217\n",
      "3807\n",
      "3807 process took: 3.835401773452759\n",
      "3808\n",
      "3808 process took: 3.718350887298584\n",
      "3838\n",
      "3838 process took: 3.257901668548584\n",
      "3889\n",
      "3889 process took: 3.1839325428009033\n",
      "3890\n",
      "3890 process took: 3.4003307819366455\n",
      "3915\n",
      "3915 process took: 2.6666512489318848\n",
      "3916\n",
      "3916 process took: 3.4892919063568115\n",
      "3918\n",
      "3918 process took: 3.242366075515747\n",
      "3920\n",
      "3920 process took: 3.571321487426758\n",
      "3923\n",
      "3923 process took: 3.267245292663574\n",
      "3926\n",
      "3926 process took: 4.2599616050720215\n",
      "3953\n",
      "3953 process took: 4.893233299255371\n",
      "3954\n",
      "3954 process took: 5.013114929199219\n",
      "3998\n",
      "3998 process took: 2.9503509998321533\n",
      "4023\n",
      "4023 process took: 4.667205095291138\n",
      "4024\n",
      "4024 process took: 4.356115341186523\n",
      "4028\n",
      "4028 process took: 4.340584993362427\n",
      "4041\n",
      "4041 process took: 3.1409361362457275\n",
      "4042\n",
      "4042 process took: 3.0450117588043213\n",
      "4045\n",
      "4045 process took: 2.8424787521362305\n",
      "4058\n",
      "4058 process took: 5.928958177566528\n",
      "4102\n",
      "4102 process took: 3.4038093090057373\n",
      "4103\n",
      "4103 process took: 3.2434606552124023\n",
      "4127\n",
      "4127 process took: 3.037202835083008\n",
      "4128\n",
      "4128 process took: 2.9582645893096924\n",
      "4130\n",
      "4130 process took: 2.6413679122924805\n",
      "4131\n",
      "4131 process took: 3.346045732498169\n",
      "4132\n",
      "4132 process took: 3.782287359237671\n",
      "4134\n",
      "4134 process took: 4.434174299240112\n",
      "4135\n",
      "4135 process took: 4.138672113418579\n",
      "4136\n",
      "4136 process took: 3.751297950744629\n",
      "4164\n",
      "4164 process took: 4.872311592102051\n",
      "4177\n",
      "missing data at station file: \u001b[0m \u001b[31m4177\u001b[0m\n",
      "4178\n",
      "missing data at station file: \u001b[0m \u001b[31m4178\u001b[0m\n",
      "4210\n",
      "missing data at station file: \u001b[0m \u001b[31m4210\u001b[0m\n",
      "4211\n",
      "missing data at station file: \u001b[0m \u001b[31m4211\u001b[0m\n",
      "4217\n",
      "4217 process took: 3.498748540878296\n",
      "4223\n",
      "4223 process took: 3.314517021179199\n",
      "4224\n",
      "4224 process took: 3.376863956451416\n",
      "4255\n",
      "4255 process took: 3.9130706787109375\n",
      "4275\n",
      "4275 process took: 4.940229654312134\n",
      "4286\n",
      "4286 process took: 2.7966763973236084\n",
      "4287\n",
      "4287 process took: 3.843252182006836\n",
      "4316\n",
      "4316 process took: 3.9205143451690674\n",
      "4317\n",
      "4317 process took: 2.9654359817504883\n",
      "4328\n",
      "missing data at station file: \u001b[0m \u001b[31m4328\u001b[0m\n",
      "4329\n",
      "missing data at station file: \u001b[0m \u001b[31m4329\u001b[0m\n",
      "4354\n",
      "missing data at station file: \u001b[0m \u001b[31m4354\u001b[0m\n",
      "4357\n",
      "4357 process took: 3.8665287494659424\n",
      "4359\n",
      "4359 process took: 2.305293083190918\n",
      "4388\n",
      "4388 process took: 3.6352169513702393\n",
      "4389\n",
      "4389 process took: 4.740617990493774\n",
      "4393\n",
      "4393 process took: 3.6649293899536133\n",
      "4394\n",
      "4394 process took: 3.5324411392211914\n",
      "4406\n",
      "4406 process took: 3.3358869552612305\n",
      "4407\n",
      "4407 process took: 6.4414308071136475\n",
      "4412\n",
      "4412 process took: 1.8659050464630127\n",
      "4464\n",
      "4464 process took: 2.5288009643554688\n",
      "4465\n",
      "4465 process took: 2.925457715988159\n",
      "4473\n",
      "4473 process took: 1.8368964195251465\n",
      "4491\n",
      "4491 process took: 1.782270908355713\n",
      "4510\n",
      "4510 process took: 2.434439182281494\n",
      "4517\n",
      "4517 process took: 2.240807056427002\n",
      "4518\n",
      "4518 process took: 1.666656255722046\n",
      "4521\n",
      "4521 process took: 2.429896116256714\n",
      "4530\n",
      "4530 process took: 1.9163753986358643\n",
      "4642\n",
      "4642 process took: 3.307502508163452\n",
      "4643\n",
      "4643 process took: 3.300936460494995\n",
      "4669\n",
      "4669 process took: 3.084042549133301\n",
      "4674\n",
      "4674 process took: 3.592085838317871\n",
      "4728\n",
      "missing data at station file: \u001b[0m \u001b[31m4728\u001b[0m\n",
      "4729\n",
      "missing data at station file: \u001b[0m \u001b[31m4729\u001b[0m\n",
      "4811\n",
      "4811 process took: 5.1544029712677\n",
      "4815\n",
      "4815 process took: 3.3088088035583496\n",
      "4839\n",
      "4839 process took: 3.704664945602417\n",
      "4847\n",
      "4847 process took: 3.3999528884887695\n",
      "4903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4903 process took: 3.234149217605591\n",
      "4905\n",
      "4905 process took: 2.9544413089752197\n",
      "5597\n",
      "5597 process took: 3.7223784923553467\n",
      "5734\n",
      "missing data at station file: \u001b[0m \u001b[31m5734\u001b[0m\n",
      "5839\n",
      "5839 process took: 3.3950858116149902\n",
      "5845\n",
      "5845 process took: 3.227142572402954\n",
      "5846\n",
      "5846 process took: 3.067761182785034\n",
      "5854\n",
      "5854 process took: 2.3334593772888184\n",
      "5882\n",
      "5882 process took: 2.993346929550171\n",
      "5905\n",
      "5905 process took: 2.603724718093872\n",
      "5906\n",
      "5906 process took: 3.8130764961242676\n",
      "5908\n",
      "5908 process took: 3.440537691116333\n",
      "5917\n",
      "5917 process took: 3.032792568206787\n",
      "5918\n",
      "5918 process took: 1.992537260055542\n",
      "5929\n",
      "5929 process took: 3.0197556018829346\n",
      "5942\n",
      "5942 process took: 2.9387855529785156\n",
      "6067\n",
      "6067 process took: 1.7825028896331787\n",
      "6068\n",
      "6068 process took: 1.7844629287719727\n",
      "6070\n",
      "6070 process took: 1.8305199146270752\n",
      "6071\n",
      "6071 process took: 1.7862768173217773\n",
      "6154\n",
      "missing data at station file: \u001b[0m \u001b[31m6154\u001b[0m\n",
      "6170\n",
      "missing data at station file: \u001b[0m \u001b[31m6170\u001b[0m\n",
      "6173\n",
      "6173 process took: 1.8950755596160889\n",
      "6176\n",
      "missing data at station file: \u001b[0m \u001b[31m6176\u001b[0m\n",
      "6184\n",
      "6184 process took: 3.0205252170562744\n",
      "6192\n",
      "6192 process took: 2.491921901702881\n",
      "6296\n",
      "6296 process took: 1.9111361503601074\n",
      "6505\n",
      "6505 process took: 0.7360401153564453\n",
      "6954\n",
      "6954 process took: 2.435152053833008\n",
      "6957\n",
      "6957 process took: 2.487881660461426\n",
      "6968\n",
      "6968 process took: 2.4417076110839844\n",
      "7070\n",
      "7070 process took: 2.640434741973877\n",
      "7079\n",
      "7079 process took: 3.138165235519409\n",
      "7090\n",
      "7090 process took: 2.4983015060424805\n",
      "7129\n",
      "7129 process took: 3.3641140460968018\n",
      "7138\n",
      "7138 process took: 2.7535288333892822\n",
      "7150\n",
      "7150 process took: 2.6249778270721436\n",
      "7172\n",
      "7172 process took: 2.396404504776001\n",
      "7185\n",
      "7185 process took: 2.0596776008605957\n"
     ]
    }
   ],
   "source": [
    "errors = sort_process_ave('./data/Targetted-GDOT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['15886',\n",
    " '202',\n",
    " '3157',\n",
    " '3232',\n",
    " '4177', #off line\n",
    " '4178',\n",
    " '4210',\n",
    " '4211',\n",
    " '4328',\n",
    " '4329',\n",
    " '4354',\n",
    " '4728',\n",
    " '4729', #off line \n",
    " '5734', #off line\n",
    " '6154', #off line /one week\n",
    " '6170', #off line\n",
    " '6176', #off line\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current version for single file\n",
    "start = time.time()\n",
    "# join 37 files into a single df\n",
    "station = '2769'\n",
    "ddf = dd.read_csv(f'./data/csv/{station}/{station}_*.csv')\n",
    "ddf = ddf.repartition(npartitions=8)\n",
    "\n",
    "# get vName for csv label\n",
    "name = (ddf['vName'].head(1)[0].split()[1])\n",
    "\n",
    "# selecting all numeric cols\n",
    "lane_type_idx = list(ddf.columns).index(\"lane_type\")\n",
    "data_names = list(ddf.columns[(lane_type_idx+1):])\n",
    "\n",
    "# map columns and create new column\n",
    "ddf.datestamp = ddf.datestamp.map(lambda x: date2Mins(x), meta=pd.Series([], dtype=str, name='x'))\n",
    "ddf.timestamp = ddf.timestamp.map(lambda x: time2Mins(x), meta=pd.Series([], dtype=str, name='x'))\n",
    "ddf = ddf.map_partitions(lambda df: df.assign(time_anchor=(df.datestamp + df.timestamp)))\n",
    "ddf = ddf.groupby('time_anchor')[data_names].mean()\n",
    "\n",
    "# # df can fit in mem so use persist\n",
    "# ddf = ddf.persist()\n",
    "# end = time.time()\n",
    "# print(f'1st process took: {end-start}')\n",
    "\n",
    "# start = time.time()\n",
    "# ddf.to_csv(f'./data/{name}_*.csv')\n",
    "# end = time.time()\n",
    "# print(f'2nd process took: {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folders in folder:\n",
    "    prnt(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single file processor for pandas\n",
    "start = time.time()\n",
    "stations_dir = './data/batched/errored2'\n",
    "for station in os.listdir(stations_dir):\n",
    "    file_path = f'{stations_dir}/{station}'\n",
    "    # join 37 files into a single df\n",
    "    dfs = []\n",
    "    for file in os.listdir(file_path):\n",
    "        df = pd.read_csv(f'{file_path}/{file}')\n",
    "        dfs.append(df)\n",
    "        \n",
    "    df = pd.concat(dfs)\n",
    "    # map columns and create new column\n",
    "    try:\n",
    "        df.datestamp = df.datestamp.map(lambda x: date2Mins(x))\n",
    "        df.timestamp = df.timestamp.map(lambda x: time2Mins(x))\n",
    "        df['time_anchor'] = df.datestamp + df.timestamp\n",
    "        # find the number of lanes of the road and split the data\n",
    "        items = list(df['lane_type'].unique())\n",
    "        for lane in items:\n",
    "            df[df['lane_type'] == lane].to_csv(f'./data/batched/dump_pd/{station}_{lane}.csv')\n",
    "    except:\n",
    "        print(f'error at: {station}_{lane}.csv')\n",
    "end = time.time()\n",
    "print(f'the process took: {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single file processor\n",
    "import os\n",
    "start = time.time()\n",
    "stations_dir = './data/batched/errored2'\n",
    "for station in os.listdir(stations_dir):\n",
    "    file_path = f'{stations_dir}/{station}'\n",
    "    # join 37 files into a single df\n",
    "    ddf = dd.read_csv(f'{file_path}/*.csv')\n",
    "    ddf = ddf.repartition(npartitions=4)\n",
    "    # map columns and create new column\n",
    "    try:\n",
    "        ddf.datestamp = ddf.datestamp.map(lambda x: date2Mins(x), meta=pd.Series([], dtype=str, name='x'))\n",
    "        ddf.timestamp = ddf.timestamp.map(lambda x: time2Mins(x), meta=pd.Series([], dtype=str, name='x'))\n",
    "        ddf = ddf.map_partitions(lambda df: df.assign(time_anchor=(df.datestamp + df.timestamp)))\n",
    "        # df can fit in mem so use persist\n",
    "    except:\n",
    "        print(f'error with file: {station}')\n",
    "        continue\n",
    "    ddf = ddf.persist()\n",
    "    end = time.time()\n",
    "    print(f'1st process took: {end-start}')\n",
    "    items = list(ddf['lane_type'].unique())\n",
    "    for lane in items:\n",
    "        ddf[ddf['lane_type'] == lane].to_csv(f'./data/batched/dump_dask/{station}_{lane}_part*.csv')\n",
    "end = time.time()\n",
    "print(f'2nd process took: {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
